import pandas as pd
import numpy as np
import re
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
import os
import urllib.request


def filter_unnecessary_rows(df):
    # ë¯¼ê°„ì£¼íƒë§Œ í•„í„°ë§ (ê³µê³µì£¼íƒì€ ê°€ì ì œ ì—†ìŒ)
    df = df[df["ì£¼íƒìƒì„¸êµ¬ë¶„ì½”ë“œëª…"] != "êµ­ë¯¼"]

    # ì„œìš¸, ê²½ê¸°, ì¸ì²œ ì§€ì—­ë§Œ í•„í„°ë§
    df = df[df["ê³µê¸‰ì§€ì—­ëª…"].isin(["ì„œìš¸", "ê²½ê¸°", "ì¸ì²œ"])]

    return df


def filter_unnecessary_columns(df):
    # ë¶ˆí•„ìš” ì¹¼ëŸ¼ í™•ì¸
    unnecessary_columns = []

    # Case 1) ì¹¼ëŸ¼ ê°’ì´ í•˜ë‚˜ë¼ë©´ ë¶ˆí•„ìš” ì¹¼ëŸ¼
    for column in df.columns:
        unnecessary_columns.extend(['ì£¼íƒêµ¬ë¶„ì½”ë“œ', 'ì£¼íƒêµ¬ë¶„ì½”ë“œëª…', 'ì£¼íƒìƒì„¸êµ¬ë¶„ì½”ë“œ', 'ì£¼íƒìƒì„¸êµ¬ë¶„ì½”ë“œëª…', 'ë¶„ì–‘êµ¬ë¶„ì½”ë“œ', 'ë¶„ì–‘êµ¬ë¶„ì½”ë“œëª…'])

    # Case 2) í™ˆí˜ì´ì§€ ì£¼ì†Œ, ë¬¸ì˜ì²˜ ì¹¼ëŸ¼
    for column in df.columns:
        if column in ["í™ˆí˜ì´ì§€ì£¼ì†Œ", "ëª¨ì§‘ê³µê³ í™ˆí˜ì´ì§€ì£¼ì†Œ", "ë¬¸ì˜ì²˜"]:
            unnecessary_columns.append(column)

    # Case 3) íŠ¹ë³„ê³µê¸‰ ê´€ë ¨ ì¹¼ëŸ¼
    for column in df.columns:
        if "íŠ¹ë³„ê³µê¸‰" in column:
            unnecessary_columns.append(column)

    # Case 4) ì²­ì•½ì ‘ìˆ˜ì‹œì‘ì¼, ì²­ì•½ì ‘ìˆ˜ì¢…ë£Œì¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì¼ì ì¹¼ëŸ¼
    for column in df.columns:
        if not (column == "ì²­ì•½ì ‘ìˆ˜ì¢…ë£Œì¼" or column == "ì²­ì•½ì ‘ìˆ˜ì‹œì‘ì¼"):
            if "ì‹œì‘ì¼" in column or "ì¢…ë£Œì¼" in column:
                unnecessary_columns.append(column)

    # Case 5) ê·¸ ì™¸ ê¸°íƒ€ ì¹¼ëŸ¼
    unnecessary_columns.extend(["ì£¼íƒê´€ë¦¬ë²ˆí˜¸", "ì£¼íƒê´€ë¦¬ë²ˆí˜¸", "ì…ì£¼ì˜ˆì •ì›”"])

    # Case 5-1) ê¸°íƒ€ ì¹¼ëŸ¼ ì¶”ê°€ ì œê±°
    unnecessary_columns.extend(["ê±´ì„¤ì—…ì²´ëª…_ì‹œê³µì‚¬", "ì‚¬ì—…ì£¼ì²´ëª…_ì‹œí–‰ì‚¬", "ê¸°ì‚¬ ë²ˆí˜¸", "ì£¼ìš” í† í”½"])

    # Case 6) ì¤‘ë³µëœ ì¹¼ëŸ¼ ì œê±°
    unnecessary_columns = list(set(unnecessary_columns))

    # ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ ì‚­ì œ
    df = df.drop(columns=unnecessary_columns)

    return df


def split_housing_type(df):
    if "ì£¼íƒí˜•" in df.columns:

        # ì£¼íƒí˜• ì—´ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬
        df['ì „ìš©ë©´ì '] = df['ì£¼íƒí˜•'].apply(lambda x: ''.join(filter(lambda y: y.isdigit() or y == '.', x)))
        df['ì „ìš©ë©´ì '] = df['ì „ìš©ë©´ì '].astype(float)
        
        # Drop the original 'ì£¼íƒí˜•' column
        # [ê´‘ì¼] ê³µê¸‰ê¸ˆì•¡ ì¹¼ëŸ¼ ì¶”ê°€ ì‹œ ì£¼íƒí˜• ì»¬ëŸ¼ì´ í•„ìš”í•´ì„œ ì œê±° ë³´ë¥˜
        # df = df.drop(columns=['ì£¼íƒí˜•'])

    return df


def preprocessing_applicant_rate(df):
    # ê²½ìŸë¥ ì´ '-'ì¸ ê²½ìš° NaNìœ¼ë¡œ ë³€í™˜
    df["ê²½ìŸë¥ "] = df["ê²½ìŸë¥ "].apply(lambda x: np.nan if str(x).strip() == "-" else x)

    def process_rate(row):
        # ê²½ìŸë¥ ì´ NaNì¸ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
        if pd.isna(row["ê²½ìŸë¥ "]):
            rate = 0.0
        # ë¯¸ë‹¬ì¸ ê²½ìš° ê²½ìŸë¥  ì²˜ë¦¬
        elif "â–³" in str(row["ê²½ìŸë¥ "]):
            pattern = "[^0-9]"
            shortage = int(re.sub(pattern, "", str(row["ê²½ìŸë¥ "])))

            try:
                supply_units = int(float(row["ê³µê¸‰ì„¸ëŒ€ìˆ˜"]))  # ê³µê¸‰ì„¸ëŒ€ìˆ˜ ìˆ«ìë¡œ ë³€í™˜
            except ValueError:
                supply_units = 0  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •

            if supply_units == 0 or int(row["ì ‘ìˆ˜ê±´ìˆ˜"]) == 0:
                rate = 0.0
            else:
                rate = round((supply_units - shortage) / supply_units, 2)
        else:
            rate = float(str(row["ê²½ìŸë¥ "]).replace(",", ""))

        # ë¯¸ë‹¬ì—¬ë¶€ íŒë‹¨
        shortage_status = "Y" if rate < 1 else "N"

        return pd.Series({"ê²½ìŸë¥ ": rate, "ë¯¸ë‹¬ì—¬ë¶€": shortage_status})

    # ê³µê¸‰ì„¸ëŒ€ìˆ˜ì™€ ì ‘ìˆ˜ê±´ìˆ˜ë¥¼ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì ìš©
    df["ê³µê¸‰ì„¸ëŒ€ìˆ˜"] = (
        pd.to_numeric(df["ê³µê¸‰ì„¸ëŒ€ìˆ˜"], errors="coerce").fillna(0).astype(int)
    )
    df["ì ‘ìˆ˜ê±´ìˆ˜"] = (
        pd.to_numeric(df["ì ‘ìˆ˜ê±´ìˆ˜"], errors="coerce").fillna(0).astype(int)
    )

    df[["ê²½ìŸë¥ ", "ë¯¸ë‹¬ì—¬ë¶€"]] = df.apply(process_rate, axis=1)

    return df


def fill_nan_with_zero(df):
    df["ê²½ìŸë¥ "] = df["ê²½ìŸë¥ "].fillna(0)
    return df


def add_estate_price(df):
    try:
        # âœ… GitHub ì›ê²© íŒŒì¼ URL (í•œê¸€ í¬í•¨ëœ íŒŒì¼ëª… ì¸ì½”ë”©)
        base_url = "https://raw.githubusercontent.com/choikwangil95/HKToss-MLOps-Proejct/streamlit/src/storage/raw_data/"
        file_name = "ì²­ì•½ë§¤ë¬¼_ê³µê¸‰ê¸ˆì•¡ (ì„œìš¸, ê²½ê¸°, ì¸ì²œ).csv"

        # âœ… í•œê¸€ URL ì¸ì½”ë”© ì²˜ë¦¬
        encoded_file_name = urllib.parse.quote(file_name)
        csv_url = base_url + encoded_file_name

        # âœ… ë¡œì»¬ íŒŒì¼ ì €ì¥ ê²½ë¡œ
        csv_path = f"./storage/raw_data/{file_name}"

        # âœ… í´ë” í™•ì¸ ë° ìƒì„±
        if not os.path.exists("./storage/raw_data"):
            os.makedirs("./storage/raw_data")

        # âœ… CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ
        if not os.path.exists(csv_path):
            print(f"ğŸ”½ CSV ë°ì´í„°ë¥¼ GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘: {csv_url}")
            urllib.request.urlretrieve(csv_url, csv_path)
            print("âœ… CSV ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")

        # âœ… CSV íŒŒì¼ ë¡œë“œ (ì¸ì½”ë”© ì˜¤ë¥˜ ëŒ€ë¹„)
        try:
            df_estate_price = pd.read_csv(csv_path, encoding="cp949")
        except UnicodeDecodeError:
            print("âš ï¸ `cp949` ì¸ì½”ë”© ì˜¤ë¥˜ ë°œìƒ â†’ `utf-8-sig`ë¡œ ì¬ì‹œë„")
            df_estate_price = pd.read_csv(csv_path, encoding="utf-8-sig")

        # âœ… í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€
        df_estate_price = df_estate_price[["ê³µê³ ë²ˆí˜¸", "ì£¼íƒí˜•", "ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ ê¸°ì¤€)"]]

        # âœ… ì¤‘ë³µ ì œê±°
        df_estate_price.drop_duplicates(subset=["ê³µê³ ë²ˆí˜¸", "ì£¼íƒí˜•"], keep="first", inplace=True)

        # âœ… ì›ë³¸ ë°ì´í„°ì— ê³µê¸‰ê¸ˆì•¡ ì¹¼ëŸ¼ ì¶”ê°€
        df = pd.merge(df, df_estate_price, on=["ê³µê³ ë²ˆí˜¸", "ì£¼íƒí˜•"], how="left")

    except Exception as e:
        print(f"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return df


import os
import urllib.request
import urllib.parse
import pandas as pd

def add_estate_list(df):
    try:
        # âœ… GitHub ì›ê²© íŒŒì¼ URL (í•œê¸€ í¬í•¨ëœ íŒŒì¼ëª… ì¸ì½”ë”©)
        base_url = "https://raw.githubusercontent.com/choikwangil95/HKToss-MLOps-Proejct/streamlit/src/storage/raw_data/"
        file_name = "ì²­ì•½ ë§¤ë¬¼ ì£¼ì†Œë³€í™˜.csv"

        # âœ… í•œê¸€ URL ì¸ì½”ë”© ì²˜ë¦¬
        encoded_file_name = urllib.parse.quote(file_name)
        csv_url = base_url + encoded_file_name

        # âœ… ë¡œì»¬ íŒŒì¼ ì €ì¥ ê²½ë¡œ
        csv_path = f"./storage/raw_data/{file_name}"

        # âœ… í´ë” í™•ì¸ ë° ìƒì„±
        if not os.path.exists("./storage/raw_data"):
            os.makedirs("./storage/raw_data")

        # âœ… CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ
        if not os.path.exists(csv_path):
            print(f"ğŸ”½ CSV ë°ì´í„°ë¥¼ GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘: {csv_url}")
            urllib.request.urlretrieve(csv_url, csv_path)
            print("âœ… CSV ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")

        # âœ… CSV íŒŒì¼ ë¡œë“œ (ì¸ì½”ë”© ì˜¤ë¥˜ ëŒ€ë¹„)
        try:
            df_estate_list = pd.read_csv(csv_path, encoding="cp949")
        except UnicodeDecodeError:
            print("âš ï¸ `cp949` ì¸ì½”ë”© ì˜¤ë¥˜ ë°œìƒ â†’ `utf-8-sig`ë¡œ ì¬ì‹œë„")
            df_estate_list = pd.read_csv(csv_path, encoding="utf-8-sig")

        # âœ… í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€
        df_estate_list = df_estate_list[
            [
                "ê³µê³ ë²ˆí˜¸",
                "ìœ„ë„",
                "ê²½ë„",
                "í–‰ì •ë™ì½”ë“œ",
                "ë²•ì •ë™ì½”ë“œ",
                "ì‹œë„",
                "ì‹œêµ°êµ¬",
                "ìë©´ë™1",
                "ìë©´ë™2",
            ]
        ]

        # âœ… ì›ë³¸ ë°ì´í„°ì— ì£¼ì†Œ ì •ë³´ ë³‘í•©
        df = pd.merge(df, df_estate_list, on="ê³µê³ ë²ˆí˜¸", how="left")

    except Exception as e:
        print(f"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return df


# ì‹œì„¸ì°¨ìµ ë°ì´í„° ì¶”ê°€
import os
import urllib.request
import urllib.parse
import pandas as pd
import numpy as np

def add_market_profit(df):
    try:
        # âœ… GitHub ì›ê²© íŒŒì¼ URL (í•œê¸€ í¬í•¨ëœ íŒŒì¼ëª… ì¸ì½”ë”©)
        base_url = "https://raw.githubusercontent.com/choikwangil95/HKToss-MLOps-Proejct/streamlit/src/storage/raw_data/"
        file_name = "ì„œìš¸ê²½ê¸°ì¸ì²œ_ì „ì²´_ì›”ë³„_ë²•ì •ë™ë³„_ì‹¤ê±°ë˜ê°€_í‰ê· .csv"

        # âœ… í•œê¸€ URL ì¸ì½”ë”© ì²˜ë¦¬
        encoded_file_name = urllib.parse.quote(file_name)
        csv_url = base_url + encoded_file_name

        # âœ… ë¡œì»¬ íŒŒì¼ ì €ì¥ ê²½ë¡œ
        csv_path = f"./storage/raw_data/{file_name}"

        # âœ… í´ë” í™•ì¸ ë° ìƒì„±
        if not os.path.exists("./storage/raw_data"):
            os.makedirs("./storage/raw_data") 

        # âœ… CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ
        if not os.path.exists(csv_path):
            try:
                print(f"ğŸ”½ CSV ë°ì´í„°ë¥¼ GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘: {csv_url}")
                urllib.request.urlretrieve(csv_url, csv_path)
                print("âœ… CSV ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                print(f"ğŸš¨ CSV ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                return df  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ë°˜í™˜

        # âœ… CSV íŒŒì¼ ë¡œë“œ (ì¸ì½”ë”© ì˜¤ë¥˜ ëŒ€ë¹„)
        try:
            df_real_estate_price = pd.read_csv(csv_path, encoding="cp949")
        except UnicodeDecodeError:
            print("âš ï¸ `cp949` ì¸ì½”ë”© ì˜¤ë¥˜ ë°œìƒ â†’ `utf-8-sig`ë¡œ ì¬ì‹œë„")
            df_real_estate_price = pd.read_csv(csv_path, encoding="utf-8-sig")

        # âœ… ëª¨ì§‘ê³µê³ ì¼ì„ ë…„ì›” í˜•íƒœë¡œ ë³€í™˜
        df['ëª¨ì§‘ê³µê³ ì¼_ë…„ì›”'] = pd.to_datetime(df['ëª¨ì§‘ê³µê³ ì¼'], errors='coerce').dt.strftime('%Y%m').astype(float)

        # âœ… ì „ìš©ë©´ì ì´ 0ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ê³„ì‚° (ZeroDivisionError ë°©ì§€)
        df['ì „ìš©ë©´ì ë‹¹ ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ê¸°ì¤€)'] = np.where(
            df['ì „ìš©ë©´ì '] > 0,
            df['ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ ê¸°ì¤€)'] / df['ì „ìš©ë©´ì '],
            0  # ì „ìš©ë©´ì ì´ 0ì´ë©´ ê¸°ë³¸ê°’ 0ìœ¼ë¡œ ì„¤ì •
        )

        # âœ… ì‹œì„¸ì°¨ìµ ê³„ì‚°ì„ ìœ„í•´ ë§¤ë¬¼ ë°ì´í„°ì™€ ì‹¤ê±°ë˜ê°€ ë°ì´í„° ë³‘í•© (ì†ë„ ìµœì í™”)
        df = df.merge(df_real_estate_price, left_on=['ë²•ì •ë™ì½”ë“œ', 'ëª¨ì§‘ê³µê³ ì¼_ë…„ì›”'],
                      right_on=['ë²•ì •ë™ì½”ë“œ', 'ë…„ì›”'], how='left')

        # âœ… ì‹œì„¸ì°¨ìµ ê³„ì‚° (NaN ë°©ì§€)
        df['ì „ìš©ë©´ì ë‹¹ ê±°ë˜ê¸ˆì•¡(ë§Œì›)'] = df['ì „ìš©ë©´ì ë‹¹ ê±°ë˜ê¸ˆì•¡(ë§Œì›)'].fillna(0)  # NaN ê°’ 0ìœ¼ë¡œ ë³€í™˜
        df['ì „ìš©ë©´ì ë‹¹ ì‹œì„¸ì°¨ìµ'] = df['ì „ìš©ë©´ì ë‹¹ ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ê¸°ì¤€)'] - df['ì „ìš©ë©´ì ë‹¹ ê±°ë˜ê¸ˆì•¡(ë§Œì›)']

        # âœ… ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ ì •ë¦¬ (NaN ë°©ì§€)
        df.drop(columns=['ëª¨ì§‘ê³µê³ ì¼_ë…„ì›”', 'ë…„ì›”', 'ì „ìš©ë©´ì ë‹¹ ê±°ë˜ê¸ˆì•¡(ë§Œì›)'], inplace=True, errors='ignore')

    except Exception as e:
        print(f"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return df



def feature_pre(df, type):

    """
    ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬ í•¨ìˆ˜
    - ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì‚­ì œ
    - í‰ê· ë‹¹ì²¨ê°€ì  ê²°ì¸¡ê°’ ì²˜ë¦¬ ë° ë°ì´í„° íƒ€ì… ë³€í™˜
    """

    # ì‚­ì œí•  ì»¬ëŸ¼ ì›ë³¸ ëª©ë¡
    # drop_cols = [

    #     'ê³µê¸‰ì§€ì—­ëª…', 'ê³µê¸‰ìœ„ì¹˜ìš°í¸ë²ˆí˜¸', 'ê³µê¸‰ìœ„ì¹˜', 'ê³µê³ ë²ˆí˜¸', 'ì£¼íƒëª…',
    #     'ëª¨ì§‘ê³µê³ ì¼', 'ì²­ì•½ì ‘ìˆ˜ì‹œì‘ì¼', 'ì²­ì•½ì ‘ìˆ˜ì¢…ë£Œì¼', 'ë‹¹ì²¨ìë°œí‘œì¼', 
    #     'ì£¼íƒí˜•', 
    #     'í‰ê· ë‹¹ì²¨ê°€ì ', 'ìµœê³ ë‹¹ì²¨ê°€ì ', 
    #     'ìœ„ë„', 'ê²½ë„', 
    #     'í–‰ì •ë™ì½”ë“œ', 'ì‹œë„', 'ì‹œêµ°êµ¬', 'ìë©´ë™1', 'ìë©´ë™2', 
    #     'ì „ìš©ë©´ì ë‹¹ ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ê¸°ì¤€)', 'ë¯¸ë‹¬ì—¬ë¶€'
    # ]
    
    drop_cols = [

        'ê³µê¸‰ì§€ì—­ëª…', 'ê³µê¸‰ìœ„ì¹˜ìš°í¸ë²ˆí˜¸', 'ê³µê¸‰ìœ„ì¹˜', 'ê³µê³ ë²ˆí˜¸', 'ì£¼íƒëª…', 
        'ëª¨ì§‘ê³µê³ ì¼', 'ì²­ì•½ì ‘ìˆ˜ì‹œì‘ì¼', 'ì²­ì•½ì ‘ìˆ˜ì¢…ë£Œì¼', 'ë‹¹ì²¨ìë°œí‘œì¼', 
        'ì£¼íƒí˜•', 'í‰ê· ë‹¹ì²¨ê°€ì ', 'ìµœê³ ë‹¹ì²¨ê°€ì ','êµ¬', 'ë²•ì •ë™', 'ë²•ì •ë™ì‹œêµ°êµ¬ì½”ë“œ', 'ë²•ì •ë™ìë©´ë™ì½”ë“œ',
        'ìœ„ë„', 'ê²½ë„', 'í–‰ì •ë™ì½”ë“œ', 'ì‹œë„', 'ì‹œêµ°êµ¬', 'ìë©´ë™1', 'ìë©´ë™2',  'ì „ìš©ë©´ì ë‹¹ ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ê¸°ì¤€)', 'ë¯¸ë‹¬ì—¬ë¶€',
        'ëŒ€ê·œëª¨íƒì§€ê°œë°œì§€êµ¬','ê±°ì£¼ì§€ì—­','ê³µê¸‰ì§€ì—­ì½”ë“œ', 'ìˆ˜ë„ê¶Œë‚´ë¯¼ì˜ê³µê³µì£¼íƒì§€êµ¬', 'ìˆœìœ„'
    ]

    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì‚­ì œ
    df.drop(drop_cols, axis=1, inplace=True)
    
    # ë‹¹ì²¨ê°€ì  ê²°ì¸¡ê°’ ì²˜ë¦¬ ë° ë°ì´í„° íƒ€ì… ë³€í™˜
    # ì´ ë¶€ë¶„ ë‚˜ì¤‘ì— ë¶ˆí•„ìš”ì‹œ í‰ê· , ìµœê³  ë“œë
    # df[['ìµœì €ë‹¹ì²¨ê°€ì ','ìµœê³ ë‹¹ì²¨ê°€ì ', 'í‰ê· ë‹¹ì²¨ê°€ì ']].fillna(0, inplace=True)
    df['ìµœì €ë‹¹ì²¨ê°€ì '].fillna(0, inplace=True)

    # df['í‰ê· ë‹¹ì²¨ê°€ì '] = df['í‰ê· ë‹¹ì²¨ê°€ì '].astype(str).str.replace("-", "0")
    # df['ìµœê³ ë‹¹ì²¨ê°€ì '] = df['ìµœê³ ë‹¹ì²¨ê°€ì '].astype(str).str.replace("-", "0")
    df['ìµœì €ë‹¹ì²¨ê°€ì '] = df['ìµœì €ë‹¹ì²¨ê°€ì '].astype(str).str.replace("-", "0")

    df['ìµœì €ë‹¹ì²¨ê°€ì '] = df['ìµœì €ë‹¹ì²¨ê°€ì '].astype(float)

    # ê²½ìŸë¥ ì´ 0ì´ê±°ë‚˜ ìµœì €ë‹¹ì²¨ê°€ì ì´ NaN ë˜ëŠ” 0ì¸ í–‰ ì‚­ì œ
    if type == 'train':
        df = df.drop(df[(df["ê²½ìŸë¥ "] == 0) | (df["ìµœì €ë‹¹ì²¨ê°€ì "].isna()) | (df["ìµœì €ë‹¹ì²¨ê°€ì "] == 0)].index)

    return df




###############################




def pipeline(type):
    # ë°ì´í„° ì „ì²˜ë¦¬
    filter_rows_transformer = FunctionTransformer(filter_unnecessary_rows)
    filter_columns_transformer = FunctionTransformer(filter_unnecessary_columns)
    split_transformer = FunctionTransformer(split_housing_type)
    rate_transformer = FunctionTransformer(preprocessing_applicant_rate)
    nan_transformer = FunctionTransformer(fill_nan_with_zero)
    price_transformer = FunctionTransformer(add_estate_price)
    list_transformer = FunctionTransformer(add_estate_list)
    profit_transformer = FunctionTransformer(add_market_profit)
    feature_transformer = FunctionTransformer(feature_pre,  kw_args={'type': type})

    # í”¼ì³ ì—”ì§€ë‹ˆì–´ë§
    # Todo: í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ ì¶”ê°€

    preprocessing_pipeline = Pipeline(
        [
            ("filter_row", filter_rows_transformer),
            ("filter_column", filter_columns_transformer),
            ("split", split_transformer),
            ("rate", rate_transformer),
            ("nan", nan_transformer),
            ("price", price_transformer),
            ("list", list_transformer),
            ('profit',  profit_transformer),
            ('feature', feature_transformer)
        ]
    )

    return preprocessing_pipeline
