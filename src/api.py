import requests
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import pandas as pd
from bs4 import BeautifulSoup
import numpy as np
import time
import urllib.parse
import re

import os
import toml
import streamlit as st
from dotenv import load_dotenv

# âœ… .env íŒŒì¼ ë¡œë“œ (ë¡œì»¬ í™˜ê²½)
load_dotenv()

# âœ… secrets.toml ë¡œë“œ (ë¡œì»¬ í™˜ê²½ë§Œ)
kakao_api_key_by_toml = None
if os.path.exists("../secrets.toml"):  # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ë¡œë“œ
    try:
        secrets = toml.load("../secrets.toml")
        kakao_api_key_by_toml = secrets.get("general", {}).get("kakao_api_key")
    except Exception as e:
        print(f"âš ï¸ Warning: secrets.tomlì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({e})")

# âœ… ìµœì¢…ì ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° (ìš°ì„ ìˆœìœ„: .env > secrets.toml > Streamlit Secrets)
kakao_api_key = (
    os.getenv("kakao_api_key") or  # âœ… ë¡œì»¬: .env ì‚¬ìš©
    kakao_api_key_by_toml or  # âœ… ë¡œì»¬: secrets.toml ì‚¬ìš©
    st.secrets.get("general", {}).get("kakao_api_key")  # âœ… Streamlit Cloud í™˜ê²½
)


# í˜„ì¬ ë‚ ì§œ
current_date = datetime.today()

# 5ë…„ ë¹¼ê¸°
past_date = current_date - relativedelta(months=60)

# YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
formatted_date = past_date.strftime("%Y-%m-%d")

def get_estate_list(area, start_date = formatted_date):
    # API URL
    url = "https://api.odcloud.kr/api/ApplyhomeInfoDetailSvc/v1/getAPTLttotPblancDetail"

    # ìš”ì²­ íŒŒë¼ë¯¸í„°
    params = {
        "page": 1,
        "perPage": 1000,
        "cond[HOUSE_SECD::EQ]": '01',  # ì£¼íƒêµ¬ë¶„ì½”ë“œ (01: ì•„íŒŒíŠ¸)
        "cond[SUBSCRPT_AREA_CODE_NM::EQ]": area, # ê³µê¸‰ì§€ì—­ëª… (100: ì„œìš¸, 400: ê²½ê¸°, 410: ì¸ì²œ)
        "cond[RCRIT_PBLANC_DE::GTE]": start_date,  # ëª¨ì§‘ê³µê³ ì¼ (ì´ìƒ)
        "serviceKey": "Zq7Jbl9Ty9DamqVoz0f+9OjZHpPVSrhzs5km2EDrccrrTShGNJVrrkNJT9//XKHOOxrlKmEAKDpoj7QpuKh4OQ=="
    }

    # ìš”ì²­ í—¤ë”
    headers = {
        "accept": "*/*"
    }

    # GET ìš”ì²­ ë³´ë‚´ê¸°
    response = requests.get(url, params=params, headers=headers)

    # ì‘ë‹µ í™•ì¸
    if response.status_code == 200:
        # print(response.json())  # JSON ë°ì´í„° ì¶œë ¥
        pass
    else:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {response.status_code}, {response.text}")

    data = response.json()['data']

    df = pd.DataFrame(data)

    # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë°ì´í„°í”„ë ˆì„ì„ ë¦¬í„´í•œë‹¤
    if df.empty:
        return df

    df = df.drop(columns=['PUBLIC_HOUSE_SPCLW_APPLC_AT'])

    getAPTLttotPblancDetail_mapping_table = {
        "HOUSE_MANAGE_NO": "ì£¼íƒê´€ë¦¬ë²ˆí˜¸",
        "PBLANC_NO": "ê³µê³ ë²ˆí˜¸",
        "HOUSE_NM": "ì£¼íƒëª…",
        "HOUSE_SECD": "ì£¼íƒêµ¬ë¶„ì½”ë“œ",
        "HOUSE_SECD_NM": "ì£¼íƒêµ¬ë¶„ì½”ë“œëª…",
        "HOUSE_DTL_SECD": "ì£¼íƒìƒì„¸êµ¬ë¶„ì½”ë“œ",
        "HOUSE_DTL_SECD_NM": "ì£¼íƒìƒì„¸êµ¬ë¶„ì½”ë“œëª…",
        "RENT_SECD": "ë¶„ì–‘êµ¬ë¶„ì½”ë“œ",
        "RENT_SECD_NM": "ë¶„ì–‘êµ¬ë¶„ì½”ë“œëª…",
        "SUBSCRPT_AREA_CODE": "ê³µê¸‰ì§€ì—­ì½”ë“œ",
        "SUBSCRPT_AREA_CODE_NM": "ê³µê¸‰ì§€ì—­ëª…",
        "HSSPLY_ZIP": "ê³µê¸‰ìœ„ì¹˜ìš°í¸ë²ˆí˜¸",
        "HSSPLY_ADRES": "ê³µê¸‰ìœ„ì¹˜",
        "TOT_SUPLY_HSHLDCO": "ê³µê¸‰ê·œëª¨",
        "RCRIT_PBLANC_DE": "ëª¨ì§‘ê³µê³ ì¼",
        "RCEPT_BGNDE": "ì²­ì•½ì ‘ìˆ˜ì‹œì‘ì¼",
        "RCEPT_ENDDE": "ì²­ì•½ì ‘ìˆ˜ì¢…ë£Œì¼",
        "SPSPLY_RCEPT_BGNDE": "íŠ¹ë³„ê³µê¸‰ì ‘ìˆ˜ì‹œì‘ì¼",
        "SPSPLY_RCEPT_ENDDE": "íŠ¹ë³„ê³µê¸‰ì ‘ìˆ˜ì¢…ë£Œì¼",
        "GNRL_RNK1_CRSPAREA_RCPTDE": "í•´ë‹¹ì§€ì—­1ìˆœìœ„ì ‘ìˆ˜ì‹œì‘ì¼",
        "GNRL_RNK1_CRSPAREA_ENDDE": "í•´ë‹¹ì§€ì—­1ìˆœìœ„ì ‘ìˆ˜ì¢…ë£Œì¼",
        "GNRL_RNK1_ETC_GG_RCPTDE": "ê²½ê¸°ì§€ì—­1ìˆœìœ„ì ‘ìˆ˜ì‹œì‘ì¼",
        "GNRL_RNK1_ETC_GG_ENDDE": "ê²½ê¸°ì§€ì—­1ìˆœìœ„ì ‘ìˆ˜ì¢…ë£Œì¼",
        "GNRL_RNK1_ETC_AREA_RCPTDE": "ê¸°íƒ€ì§€ì—­1ìˆœìœ„ì ‘ìˆ˜ì‹œì‘ì¼",
        "GNRL_RNK1_ETC_AREA_ENDDE": "ê¸°íƒ€ì§€ì—­1ìˆœìœ„ì ‘ìˆ˜ì¢…ë£Œì¼",
        "GNRL_RNK2_CRSPAREA_RCPTDE": "í•´ë‹¹ì§€ì—­2ìˆœìœ„ì ‘ìˆ˜ì‹œì‘ì¼",
        "GNRL_RNK2_CRSPAREA_ENDDE": "í•´ë‹¹ì§€ì—­2ìˆœìœ„ì ‘ìˆ˜ì¢…ë£Œì¼",
        "GNRL_RNK2_ETC_GG_RCPTDE": "ê²½ê¸°ì§€ì—­2ìˆœìœ„ì ‘ìˆ˜ì‹œì‘ì¼",
        "GNRL_RNK2_ETC_GG_ENDDE": "ê²½ê¸°ì§€ì—­2ìˆœìœ„ì ‘ìˆ˜ì¢…ë£Œì¼",
        "GNRL_RNK2_ETC_AREA_RCPTDE": "ê¸°íƒ€ì§€ì—­2ìˆœìœ„ì ‘ìˆ˜ì‹œì‘ì¼",
        "GNRL_RNK2_ETC_AREA_ENDDE": "ê¸°íƒ€ì§€ì—­2ìˆœìœ„ì ‘ìˆ˜ì¢…ë£Œì¼",
        "PRZWNER_PRESNATN_DE": "ë‹¹ì²¨ìë°œí‘œì¼",
        "CNTRCT_CNCLS_BGNDE": "ê³„ì•½ì‹œì‘ì¼",
        "CNTRCT_CNCLS_ENDDE": "ê³„ì•½ì¢…ë£Œì¼",
        "HMPG_ADRES": "í™ˆí˜ì´ì§€ì£¼ì†Œ",
        "CNSTRCT_ENTRPS_NM": "ê±´ì„¤ì—…ì²´ëª…_ì‹œê³µì‚¬",
        "MDHS_TELNO": "ë¬¸ì˜ì²˜",
        "BSNS_MBY_NM": "ì‚¬ì—…ì£¼ì²´ëª…_ì‹œí–‰ì‚¬",
        "MVN_PREARNGE_YM": "ì…ì£¼ì˜ˆì •ì›”",
        "SPECLT_RDN_EARTH_AT": "íˆ¬ê¸°ê³¼ì—´ì§€êµ¬",
        "MDAT_TRGET_AREA_SECD": "ì¡°ì •ëŒ€ìƒì§€ì—­",
        "PARCPRC_ULS_AT": "ë¶„ì–‘ê°€ìƒí•œì œ",
        "IMPRMN_BSNS_AT": "ì •ë¹„ì‚¬ì—…",
        "PUBLIC_HOUSE_EARTH_AT": "ê³µê³µì£¼íƒì§€êµ¬",
        "LRSCL_BLDLND_AT": "ëŒ€ê·œëª¨íƒì§€ê°œë°œì§€êµ¬",
        "NPLN_PRVOPR_PUBLIC_HOUSE_AT": "ìˆ˜ë„ê¶Œë‚´ë¯¼ì˜ê³µê³µì£¼íƒì§€êµ¬",
        #"PUBLIC_HOUSE_SPCLW_APPLC_AT": "ê³µê³µì£¼íƒ íŠ¹ë³„ë²• ì ìš© ì—¬ë¶€",
        "PBLANC_URL": "ëª¨ì§‘ê³µê³ í™ˆí˜ì´ì§€ì£¼ì†Œ",
    }
    df = df.rename(columns=getAPTLttotPblancDetail_mapping_table)
    df = df[getAPTLttotPblancDetail_mapping_table.values()]

    return df


def get_estate_applicant_list(area_code, start_date, end_date):
    """íŠ¹ì • ì§€ì—­ì½”ë“œ(area_code)ì™€ ì‹œì‘ ì—°ì›”(start_date)ì— ëŒ€í•´ ì²­ì•½ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    
    url = "https://api.odcloud.kr/api/ApplyhomeStatSvc/v1/getAPTReqstAreaStat"
    
    params = {
        "page": 1,
        "perPage": 1, 
        "cond[SUBSCRPT_AREA_CODE::EQ]": area_code,
        "cond[STAT_DE::GT]": start_date,  # ì‹œì‘ ì›” ì´í›„ ë°ì´í„°
        "cond[STAT_DE::LTE]": end_date,
        "serviceKey": "Zq7Jbl9Ty9DamqVoz0f+9OjZHpPVSrhzs5km2EDrccrrTShGNJVrrkNJT9//XKHOOxrlKmEAKDpoj7QpuKh4OQ=="
    }

    headers = {
        "accept": "*/*"
    }

    response = requests.get(url, params=params, headers=headers)
    
    # ì‘ë‹µ í™•ì¸
    if response.status_code != 200:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {response.status_code}, {response.text}")
        return pd.DataFrame()  # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜

    data = response.json()

    if 'data' not in data or len(data['data']) == 0:
        return pd.DataFrame()

    df = pd.DataFrame(data['data'])

    # ì»¬ëŸ¼ ë§¤í•‘ í…Œì´ë¸” ì ìš©
    estate_applicant_mapping_table = {
        "STAT_DE": "ì—°ì›”",
        "SUBSCRPT_AREA_CODE_NM": "ì‹œë„",
        "AGE_30": "30ëŒ€ ì´í•˜ ì‹ ì²­ê±´ìˆ˜",
        "AGE_40": "40ëŒ€ ì‹ ì²­ê±´ìˆ˜",
        "AGE_50": "50ëŒ€ ì‹ ì²­ê±´ìˆ˜",
        "AGE_60": "60ëŒ€ ì´ìƒ ì‹ ì²­ê±´ìˆ˜"
    }
    
    # ì»¬ëŸ¼ëª… ë³€ê²½ ë° í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€
    df = df.rename(columns=estate_applicant_mapping_table)
    df = df[estate_applicant_mapping_table.values()]

    return df


def get_estate_applicant_list_total(area_code):
    """íŠ¹ì • ì§€ì—­ì½”ë“œ(area_code)ì— ëŒ€í•´ ìµœê·¼ 5ë…„ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""

    df_applicant_list_total = pd.DataFrame()

    for i in range(61):
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ 5ë…„ ì „ê¹Œì§€ì˜ ì—°ì›” ê³„ì‚°
        current_date = datetime.today()
        start_date = (current_date - relativedelta(months=i+1)).strftime("%Y%m")
        end_date = (current_date - relativedelta(months=i)).strftime("%Y%m")

        # í•´ë‹¹ ì—°ì›”ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ëˆ„ì 
        df_applicant = get_estate_applicant_list(area_code, start_date, end_date)

        # ë°ì´í„° ì •ë ¬
        if not df_applicant.empty or not df_applicant.empty:
            df_applicant = df_applicant.sort_values(by='ì—°ì›”', ascending=False).reset_index(drop=True)
            df_applicant_list_total = pd.concat([df_applicant_list_total, df_applicant], ignore_index=True)

    return df_applicant_list_total


def get_estate_winner_list(area_code, start_date, end_date):
    """íŠ¹ì • ì§€ì—­ì½”ë“œ(area_code)ì™€ ì‹œì‘ ì—°ì›”(start_date)ì— ëŒ€í•´ ì²­ì•½ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    
    url = "https://api.odcloud.kr/api/ApplyhomeStatSvc/v1/getAPTPrzwnerAreaStat"
    
    params = {
        "page": 1,
        "perPage": 1, 
        "cond[SUBSCRPT_AREA_CODE::EQ]": area_code,
        "cond[STAT_DE::GT]": start_date,  # ì‹œì‘ ì›” ì´í›„ ë°ì´í„°
        "cond[STAT_DE::LTE]": end_date,
        "serviceKey": "Zq7Jbl9Ty9DamqVoz0f+9OjZHpPVSrhzs5km2EDrccrrTShGNJVrrkNJT9//XKHOOxrlKmEAKDpoj7QpuKh4OQ=="
    }

    headers = {
        "accept": "*/*"
    }

    response = requests.get(url, params=params, headers=headers)
    
    # ì‘ë‹µ í™•ì¸
    if response.status_code != 200:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {response.status_code}, {response.text}")
        return pd.DataFrame()  # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜

    data = response.json()

    if 'data' not in data or len(data['data']) == 0:
        return pd.DataFrame()

    df = pd.DataFrame(data['data'])

    # ì»¬ëŸ¼ ë§¤í•‘ í…Œì´ë¸” ì ìš©
    estate_applicant_mapping_table = {
        "STAT_DE": "ì—°ì›”",
        "SUBSCRPT_AREA_CODE_NM": "ì‹œë„",
        "AGE_30": "30ëŒ€ ì´í•˜ ë‹¹ì²¨ê±´ìˆ˜",
        "AGE_40": "40ëŒ€ ë‹¹ì²¨ê±´ìˆ˜",
        "AGE_50": "50ëŒ€ ë‹¹ì²¨ê±´ìˆ˜",
        "AGE_60": "60ëŒ€ ì´ìƒ ë‹¹ì²¨ê±´ìˆ˜"
    }
    
    # ì»¬ëŸ¼ëª… ë³€ê²½ ë° í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€
    df = df.rename(columns=estate_applicant_mapping_table)
    df = df[estate_applicant_mapping_table.values()]

    return df


def get_estate_winner_list_total(area_code):
    """íŠ¹ì • ì§€ì—­ì½”ë“œ(area_code)ì— ëŒ€í•´ ìµœê·¼ 5ë…„ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""

    df_applicant_list_total = pd.DataFrame()

    for i in range(61):
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ 5ë…„ ì „ê¹Œì§€ì˜ ì—°ì›” ê³„ì‚°
        current_date = datetime.today()
        start_date = (current_date - relativedelta(months=i+1)).strftime("%Y%m")
        end_date = (current_date - relativedelta(months=i)).strftime("%Y%m")

        # í•´ë‹¹ ì—°ì›”ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ëˆ„ì 
        df_applicant = get_estate_winner_list(area_code, start_date, end_date)

        # ë°ì´í„° ì •ë ¬
        if not df_applicant.empty or not df_applicant.empty:
            df_applicant = df_applicant.sort_values(by='ì—°ì›”', ascending=False).reset_index(drop=True)
            df_applicant_list_total = pd.concat([df_applicant_list_total, df_applicant], ignore_index=True)

    return df_applicant_list_total


def get_estate_detail(ID):
    URL = f"https://www.applyhome.co.kr/ai/aia/selectAPTCompetitionPopup.do?houseManageNo={ID}&pblancNo={ID}"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }

    response = requests.get(URL, headers=headers)
    text = response.text

    # BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ HTML íŒŒì‹±
    soup = BeautifulSoup(text, 'html.parser')

    # í…Œì´ë¸” ì„ íƒ
    table = soup.find("table", {"id": "compitTbl"})

    # í…Œì´ë¸” í—¤ë” ì¶”ì¶œ
    headers = ['ì£¼íƒí˜•', 'ê³µê¸‰ì„¸ëŒ€ìˆ˜', 'ìˆœìœ„', 'ê±°ì£¼ì§€ì—­', 'ì ‘ìˆ˜ê±´ìˆ˜', 'ê²½ìŸë¥ ', 'ì²­ì•½ê²°ê³¼', 'ì§€ì—­', 'ìµœì €ë‹¹ì²¨ê°€ì ', 'ìµœê³ ë‹¹ì²¨ê°€ì ', 'í‰ê· ë‹¹ì²¨ê°€ì ']

    # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
    data = []
    for row in table.find("tbody").find_all("tr"):
        cols = row.find_all("td")
        if len(cols) > 0:  # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
            data_row = [np.nan if col.text.strip() == '' else col.text.strip() for col in cols]

            # ë‹¹ì²¨ê°€ì  ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° nan ê°’ìœ¼ë¡œ ì±„ì›Œì¤€ë‹¤
            data_row.extend([np.nan] * (11 - len(data_row)))

            if data_row[0] != "ì´í•©ê³„":  # 'ì´í•©ê³„' í–‰ ì œê±°
                data.append(data_row)

    # pandas DataFrameìœ¼ë¡œ ë³€í™˜
    df_estate_detail = pd.DataFrame(data, columns=headers)
    df_estate_detail['ì£¼íƒê´€ë¦¬ë²ˆí˜¸'] = ID
    df_estate_detail['ê³µê³ ë²ˆí˜¸'] = ID
    df_estate_detail = df_estate_detail[['ì£¼íƒê´€ë¦¬ë²ˆí˜¸', 'ê³µê³ ë²ˆí˜¸', 'ì£¼íƒí˜•', 'ê³µê¸‰ì„¸ëŒ€ìˆ˜', 'ìˆœìœ„', 'ê±°ì£¼ì§€ì—­', 'ì ‘ìˆ˜ê±´ìˆ˜', 'ê²½ìŸë¥ ', 'ìµœì €ë‹¹ì²¨ê°€ì ', 'ìµœê³ ë‹¹ì²¨ê°€ì ', 'í‰ê· ë‹¹ì²¨ê°€ì ']]
    df_estate_detail

    return df_estate_detail

# ë‹¹ì²¨ê°€ì ì´ ë°œí‘œë˜ì§€ ì•Šì€ ìµœì‹  ë§¤ë¬¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
def get_future_estate_list():
    # ë§¤ë¬¼ ëª©ë¡ APIë¡œ ê°€ì ¸ì˜¤ê¸°

    from datetime import datetime, timedelta

    # api.pyê°€ ìˆëŠ” ìƒìœ„ ê²½ë¡œ ì¶”ê°€
    import pandas as pd

    # ëª¨ì§‘ê³µê³ ì¼ì´ ìµœì†Œ 2ì›”ì¸ ë§¤ë¬¼ ì°¾ê¸°
    current_date = '2025-02-01'

    df_test = pd.DataFrame()

    df_estate_1 = get_estate_list('ì„œìš¸', current_date)
    if not df_estate_1.empty:
        df_test  = pd.concat([df_test, df_estate_1]).reset_index(drop=True)

    df_estate_2 = get_estate_list('ê²½ê¸°', current_date)
    if not df_estate_2.empty:
        df_test  = pd.concat([df_test, df_estate_2]).reset_index(drop=True)

    df_estate_3 = get_estate_list('ì¸ì²œ', current_date)
    if not df_estate_3.empty:
        df_test  = pd.concat([df_test, df_estate_3]).reset_index(drop=True)

    # ë¯¼ì˜ ì£¼íƒ ë§¤ë¬¼ë§Œ í•„í„°ë§
    df_test = df_test[df_test['ì£¼íƒìƒì„¸êµ¬ë¶„ì½”ë“œëª…'] == 'ë¯¼ì˜'].reset_index(drop=True)

    # ë‹¹ì²¨ìë°œí‘œì¼ì´ í˜„ì¬ë³´ë‹¤ ë¯¸ë˜ì¸ ë§¤ë¬¼ë§Œ í•„í„°ë§
    today = datetime.now().date()
    df_test['ë‹¹ì²¨ìë°œí‘œì¼'] = pd.to_datetime(df_test['ë‹¹ì²¨ìë°œí‘œì¼']).dt.date
    df_test = df_test[df_test['ë‹¹ì²¨ìë°œí‘œì¼'] > today]

    # ì²­ì•½ ë§¤ë¬¼ ëª©ë¡ ì •ë³´ + ë‹¹ì²¨ê°€ì , ê²½ìŸë¥  ì •ë³´
    df_test_ids = df_test['ê³µê³ ë²ˆí˜¸'].unique().tolist()

    for id in df_test_ids:
        df_estate_detail = get_estate_detail(id)

        df_test = pd.merge(
            df_test,
            df_estate_detail,
            on=['ì£¼íƒê´€ë¦¬ë²ˆí˜¸', 'ê³µê³ ë²ˆí˜¸'], 
            how='inner',
        ).reset_index(drop=True)

    return df_test


def generate_news_url(apartment_name, apartment_ds, apartment_de):
    base_url = "https://search.naver.com/search.naver"
    # ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    apartment_name = re.sub(r'\([^)]*\)', '', apartment_name).strip()
    query = f'{apartment_name} ì²­ì•½'
    params = {
        "where": "news",
        "query": query,
        "sm": "tab_opt",
        "sort": 0, # ê´€ë ¨ë„ìˆœ ì •ë ¬
        "nso": f'so:r,p:from{apartment_ds}to{apartment_de}'  # ì¡°íšŒê¸°ê°„: ëª¨ì§‘ê³µê³ ì¼ ~ ë‹¹ì²¨ìë°œí‘œì¼
    }
    return base_url + "?" + urllib.parse.urlencode(params)

def crawl_naver_news(url, max_articles=3):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36",
        "Accept-Language": "ko-KR,ko;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Connection": "keep-alive",
        "Referer": "https://www.google.com/",
    }
    articles = []
    page = 1  # í˜ì´ì§€ ë²ˆí˜¸ ì´ˆê¸°í™”

    cnt = 0

    def crawl_article_content(news_url, headers):
        """ê¸°ì‚¬ URLì„ ë°›ì•„ì„œ ì „ì²´ ë‚´ìš©ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜"""
        try:
            article_response = requests.get(news_url, headers=headers)
            article_response.raise_for_status()  # HTTP ì—ëŸ¬ í™•ì¸
            article_soup = BeautifulSoup(article_response.text, 'html.parser')

            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì†Œ ì„ íƒ (ê¸°ì‚¬ì— ë”°ë¼ ì„ íƒìê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            content_element = article_soup.select_one('#newsct_article') or article_soup.select_one('#dic_area')

            if content_element:
                return content_element.get_text(strip=True)
            else:
                return "ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        except requests.exceptions.RequestException as e:
            print(f"ê¸°ì‚¬ ë‚´ìš© ìš”ì²­ ì—ëŸ¬ ë°œìƒ: {e}")
            return "ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"ê¸°ì‚¬ ë‚´ìš© íŒŒì‹± ì—ëŸ¬ ë°œìƒ: {e}")
            return "ê¸°ì‚¬ ë‚´ìš©ì„ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    while cnt < max_articles:  # ì›í•˜ëŠ” ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ
        try:
            # í˜ì´ì§€ URL ìƒì„± (í˜ì´ì§€ ë²ˆí˜¸ ì ìš©)
            paged_url = f"{url}&start={(page - 1) * 10 + 1}"
            response = requests.get(paged_url, headers=headers)
            response.raise_for_status()  # HTTP ì—ëŸ¬ í™•ì¸

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê¸°ì‚¬ ì˜ì—­ ì„ íƒ
            news_areas = soup.select(".news_area")
            
            # ê¸°ì‚¬ ì˜ì—­ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
            if not news_areas:
                print("ë” ì´ìƒ ê¸°ì‚¬ ì˜ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")
                break

            for item in news_areas:
                # "ë„¤ì´ë²„ ë‰´ìŠ¤" ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                naver_news_link = item.select_one("a[href*='news.naver.com']")
                if not naver_news_link:
                    continue  # "ë„¤ì´ë²„ ë‰´ìŠ¤" ë§í¬ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ê¸°ì‚¬ë¡œ ê±´ë„ˆëœ€
                
                title = item.select_one(".news_tit").text
                news_url = naver_news_link['href']  # ë„¤ì´ë²„ ë‰´ìŠ¤ URL ì‚¬ìš©

                # ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜ í˜¸ì¶œ
                full_content = crawl_article_content(news_url, headers)

                # í‚¤ì›Œë“œ í•„í„°ë§
                if "ì²­ì•½" in full_content:  # í¬ë¡¤ë§ëœ ì „ì²´ ë‚´ìš©ì—ì„œ "ì²­ì•½" í‚¤ì›Œë“œ í™•ì¸
                    articles.append({"title": title, "content": full_content, "url": news_url})

                cnt += 1
                print(f'{cnt}ë²ˆ ê¸°ì‚¬ - {title}')

                if (cnt >= max_articles):
                    break
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            page += 1
            time.sleep(1)  # í˜ì´ì§€ ìš”ì²­ ê°„ 3ì´ˆ ëŒ€ê¸°

        except requests.exceptions.RequestException as e:
            print(f"ìš”ì²­ ì—ëŸ¬ ë°œìƒ: {e}")
            break  # ìš”ì²­ ì—ëŸ¬ ë°œìƒ ì‹œ í¬ë¡¤ë§ ì¤‘ë‹¨
        except Exception as e:
            print(f"íŒŒì‹± ì—ëŸ¬ ë°œìƒ: {e}")
            break  # íŒŒì‹± ì—ëŸ¬ ë°œìƒ ì‹œ í¬ë¡¤ë§ ì¤‘ë‹¨

    return articles

def get_apartment_news(df):
    from api import generate_news_url, crawl_naver_news

    df_unique = df.drop_duplicates(subset='ê³µê³ ë²ˆí˜¸', keep='first').reset_index(drop=True)
    df_ids = df_unique['ê³µê³ ë²ˆí˜¸'].tolist()

    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_news_data = []

    for id in df_ids:
        df_detail = df_unique[df_unique['ê³µê³ ë²ˆí˜¸'] == '2025000021'].iloc[0]

        apartment_name = df_detail['ì£¼íƒëª…']
        apartment_ds = df_detail['ëª¨ì§‘ê³µê³ ì¼']
        apartment_de = df_detail['ë‹¹ì²¨ìë°œí‘œì¼']

        # url ìƒì„±
        url = generate_news_url(apartment_name, apartment_ds, apartment_de)

        # ê¸°ì‚¬ íƒìŠ¤íŠ¸ í¬ë¡¤ë§
        news_data = crawl_naver_news(url, max_articles=3)

        # ê²°ê³¼ ì €ì¥
        for article in news_data:
            article['ê³µê³ ë²ˆí˜¸'] = id
            article['apartment'] = apartment_name
        all_news_data.extend(news_data)

    result_df = pd.DataFrame(all_news_data)
    result_df = result_df[['ê³µê³ ë²ˆí˜¸', 'apartment', 'title', 'content', 'url']]
    result_df

    return result_df


def add_topic_keyword(df_future_estate_list):

    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    count_vectorizer_path = os.path.join(current_dir, "storage/topic_modeling/countvectorizer_model_0.0.1.pkl")
    lda_model_path = os.path.join(current_dir, "storage/topic_modeling/lda_model_0.0.1.pkl")
    stopwords_path = os.path.join(current_dir, "datasets/stopwords-ko.txt")

    # í† í”½ ëª¨ë¸ë§ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    import joblib
    count_vectorizer = joblib.load(count_vectorizer_path)
    lda_model = joblib.load(  lda_model_path)

    # í˜•íƒœì†Œ ë¶„ì„ê¸° ì„¤ì •
    from konlpy.tag import Okt
    import re
    import pandas as pd

    okt = Okt()

    # ë¶ˆìš©ì–´ ì •ë¦¬
    with open('./datasets/stopwords-ko.txt', 'r', encoding='utf-8') as f:
        list_file = f.readlines()
    stopwords_default = [word[:-1] for word in list_file ]
    stopwords_default
    stopwords = stopwords_default # ê¸°ë³¸

    # 1. í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ (íŠ¹ìˆ˜ë¬¸ì, ìˆ«ì ì œê±°)
    def clean_text(text):
        text = re.sub(r'\[.*?\]|\(.*?\)', '', text) # (), [] ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±°
        text = re.sub(r'[^ê°€-í£\s]', '', text)  # í•œê¸€ê³¼ ê³µë°± ì œì™¸ ë¬¸ì ì œê±°
        text = re.sub(r'\s+', ' ', text).strip()  # ì—°ì† ê³µë°± ì œê±°
        return text

    # 2. í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
    def extract_nouns(text):
        nouns = okt.nouns(text)  # í˜•íƒœì†Œë§Œ ì¶”ì¶œ
        nouns = [word for word in nouns if word not in stopwords and len(word) > 1]  # ë¶ˆìš©ì–´ ì œê±° ë° í•œ ê¸€ì ë‹¨ì–´ ì œì™¸
        return ' '.join(nouns)

    # ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§
    df_news = get_apartment_news(df_future_estate_list)

    # 3. ì „ì²´ ë°ì´í„° ì „ì²˜ë¦¬
    corpus = df_news['content'].tolist()
    cleaned_corpus = [extract_nouns(clean_text(text)) for text in corpus]  # ì •ì œ + ëª…ì‚¬ ì¶”ì¶œ

    dtm = count_vectorizer.transform(cleaned_corpus)  # ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ DTM ìƒì„±

    # ê° ê¸°ì‚¬ë³„ í† í”½ ë¶„í¬ (í† í”½ ì ìˆ˜) ê³„ì‚°
    doc_topic = lda_model.transform(dtm)

    topic_names = [f'í† í”½ {i}' for i in range(1, 8)]

    results = []
    for i, topic_dist in enumerate(doc_topic):
        top_topic = topic_dist.argmax()
        result = {
            'ê¸°ì‚¬ ë²ˆí˜¸': i + 1,
            'ì£¼ìš” í† í”½': topic_names[top_topic],
        }
        # ê° í† í”½ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ì†Œìˆ˜ì  4ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼í•˜ì—¬ ê²°ê³¼ì— ì¶”ê°€
        result.update({topic_names[j]: round(topic_dist[j], 4) for j in range(len(topic_names))})
        results.append(result)

    df_results = pd.DataFrame(results)

    # âœ… ê³µê³ ë²ˆí˜¸ ë§¤í•‘ (df['ê³µê³ ë²ˆí˜¸']ì™€ df_resultsë¥¼ ì¸ë±ìŠ¤ë¡œ ì—°ê²°)
    df_results['ê³µê³ ë²ˆí˜¸'] = df_news['ê³µê³ ë²ˆí˜¸'].values
    df_results = df_results[['ê³µê³ ë²ˆí˜¸', 'í† í”½ 1', 'í† í”½ 2', 'í† í”½ 3', 'í† í”½ 4', 'í† í”½ 5', 'í† í”½ 6', 'í† í”½ 7']]

    # ê¸°ì‚¬ê°€ ì—¬ëŸ¬ê°œì¸ ë§¤ë¬¼ì€ ê° ê¸°ì‚¬ë³„ í† í”½ ì ìˆ˜ì˜ í‰ê· ìœ¼ë¡œ í•œë‹¤.
    df_results = df_results.groupby('ê³µê³ ë²ˆí˜¸').mean().reset_index()

    df_future_estate_list = pd.merge(df_future_estate_list, df_results, how='inner')

    return df_future_estate_list

def add_address_by_apartname(apartname):
  kakao_api_url = "https://dapi.kakao.com/v2/local/search/keyword.json"
  headers = {"Authorization": f"KakaoAK {kakao_api_key}"}

  response = requests.get(kakao_api_url, headers=headers, params={"query": apartname})
  data = response.json()['documents']
  has_data = len(data) != 0
  
  if has_data:
    address = data[0]['address_name']
    return address
  
  return ''

def get_lat_lon_kakao(address):
    kakao_api_url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {kakao_api_key}"}

    try:
        response = requests.get(kakao_api_url, headers=headers, params={"query": address})
        # ìš”ì²­ ì‹¤íŒ¨í•œê²½ìš°
        if response.status_code != 200:
            return None
        
        result = response.json()
        data = result['documents']
        has_data = len(data) != 0

        # ì£¼ì†Œ ë°ì´í„° ì—†ëŠ”ê²½ìš°
        if not has_data:
            print(f'{address} - ì£¼ì†Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
            return None
        
        result = data[0]['address']

        region_depth_3_h = result['region_3depth_h_name'] # ë™
        region_depth_3 = result['region_3depth_name'] # ë™

        # ë™ ë°ì´í„° ì—†ëŠ” ê²½ìš°
        if region_depth_3_h == '' and region_depth_3 == '':
            print(f'{address} - ë™ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
            return None

        return result
    except Exception as e:
        print(f"Error fetching data for address {address}: {e}")
        return None

def process_address_data(address, address_by_apartname, verbose=False):
    """ì¹´ì¹´ì˜¤ë§µ APIë¥¼ ì‚¬ìš©í•´ ìœ„ë„, ê²½ë„ ë° í–‰ì •ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    
    try:
        # 1ï¸âƒ£ ì¹´ì¹´ì˜¤ë§µ API ì‚¬ìš©í•´ì„œ ì£¼ì†Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        data = get_lat_lon_kakao(address)

        # 2ï¸âƒ£ ê³µê¸‰ìœ„ì¹˜ë¡œ ì£¼ì†Œ ëª» ì–»ëŠ” ê²½ìš°, ì£¼íƒëª… ê¸°ë°˜ìœ¼ë¡œ ì¬ì‹œë„
        if not data:
            data = get_lat_lon_kakao(address_by_apartname)

        # 3ï¸âƒ£ ê·¸ë˜ë„ ì—†ëŠ” ê²½ìš° â†’ NaN ì²˜ë¦¬
        if not data:
            if verbose:
                print(f"[ì£¼ì†Œ ë³€í™˜ ì‹¤íŒ¨] '{address}', '{address_by_apartname}' â†’ NaN ê°’ ë°˜í™˜")
            return None, None, None, None, None, None, None, None  # 8ê°œ ë°˜í™˜ ë³´ì¥

        # 4ï¸âƒ£ ì •ìƒì ì¸ ë°ì´í„° ì¶”ì¶œ
        lat = data.get('y', None)
        lon = data.get('x', None)
        h_code = data.get('h_code', None)
        b_code = data.get('b_code', None)
        region_depth_1 = data.get('region_1depth_name', None)  # ë„
        region_depth_2 = data.get('region_2depth_name', None)  # ì‹œ,êµ¬
        region_depth_3_h = data.get('region_3depth_h_name', None)  # í–‰ì •ë™
        region_depth_3 = data.get('region_3depth_name', None)  # ë²•ì •ë™

        # 5ï¸âƒ£ í•„ìš”í•  ë•Œë§Œ ë¡œê·¸ ì¶œë ¥ (verbose=True ì„¤ì • ì‹œ)
        if verbose:
            print(f"[ì£¼ì†Œ ë³€í™˜ ì„±ê³µ] '{address}' â†’ {lat}, {lon}, {h_code}, {b_code}, {region_depth_1}, {region_depth_2}, {region_depth_3_h}, {region_depth_3}")

        return lat, lon, h_code, b_code, region_depth_1, region_depth_2, region_depth_3_h, region_depth_3
    
    except Exception as e:
        print(f"[ì˜¤ë¥˜ ë°œìƒ] {e} (address='{address}', address_by_apartname='{address_by_apartname}')")
        return None, None, None, None, None, None, None, None  # ì˜¤ë¥˜ ë°œìƒ ì‹œë„ ì•ˆì „í•˜ê²Œ 8ê°œ ë°˜í™˜
    
# ì•„íŒŒíŠ¸ëª… ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_apartname(df):

    # íŠ¹ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì •í™•í•œ ì¼ì¹˜ ë‹¨ì–´ ì œê±°)
    custom_stopwords = {}

    # íŠ¹ì • ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë‹¨ì–´ ì œê±° (ì˜ˆ: 'ì§€êµ¬'ê°€ í¬í•¨ëœ ë‹¨ì–´ ì œê±°)
    remove_if_contains = ['ìš°ì„ ' , 'ë¶„ì–‘' ,'í›„', 'ì”ì—¬ì„¸ëŒ€', 'ëª¨ì§‘ê³µê³ ', 'ê³µê¸‰', 'ì„¸ëŒ€', '-', 'ì „í™˜', 'ë¸”ë¡', 'ê³µê³µ']

    # âœ… ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ëª¨ë‘ ì œê±°í•˜ëŠ” í•¨ìˆ˜
    def remove_parentheses(text):
        """ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ëª¨ë‘ ì œê±°"""
        return re.sub(r'\(.*?\)', '', text).strip()

    # âœ… í•„í„°ë§ ì¡°ê±´ í•¨ìˆ˜
    def filter_conditions(word):
        """ë‹¨ì–´ê°€ ì œê±° ëŒ€ìƒì¸ì§€ í™•ì¸"""
        # if re.search(r'[A-Za-z]', word):  # ì˜ì–´ í¬í•¨ ë‹¨ì–´ ì œê±°
        #     return False
        if word in custom_stopwords:  # íŠ¹ì • í‚¤ì›Œë“œ ì œê±°
            return False
        if any(sub in word for sub in remove_if_contains):  # íŠ¹ì • ë‹¨ì–´ í¬í•¨ ë‹¨ì–´ ì œê±°
            return False
        return True  # ëª¨ë“  ì¡°ê±´ì„ í†µê³¼í•˜ë©´ ìœ ì§€

    # âœ… ì •ì œ í•¨ìˆ˜
    def clean_name(address):
        # ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±°
        address = remove_parentheses(address)

        words = address.split()  # ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        cleaned_words = [word for word in words if filter_conditions(word)]  # í•„í„°ë§ ì ìš©
        cleaned_address = ' '.join(cleaned_words)  # ì •ì œëœ ë‹¨ì–´ë“¤ ë‹¤ì‹œ í•©ì¹˜ê¸°

        # ì‰¼í‘œê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ë°˜í™˜
        if ',' in cleaned_address:
            return cleaned_address.split(',')[0]
        return cleaned_address  # ì‰¼í‘œê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    # âœ… ì£¼ì†Œ ì •ì œ ì ìš©
    df['ì •ì œëœì£¼íƒëª…'] = df['ê³µê¸‰ì§€ì—­ëª…'] + ' ' + df['ì£¼íƒëª…'].apply(clean_name)

    return df


def add_address_code(df_future_estate_list):
    df_future_estate_list_unique = df_future_estate_list.drop_duplicates(subset='ê³µê³ ë²ˆí˜¸', keep='first')
    df_future_estate_list_unique

    # ì•„íŒŒíŠ¸ëª… ì „ì²˜ë¦¬ í•¨ìˆ˜
    def preprocess_apartname(df):

        # íŠ¹ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì •í™•í•œ ì¼ì¹˜ ë‹¨ì–´ ì œê±°)
        custom_stopwords = {}

        # íŠ¹ì • ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë‹¨ì–´ ì œê±° (ì˜ˆ: 'ì§€êµ¬'ê°€ í¬í•¨ëœ ë‹¨ì–´ ì œê±°)
        remove_if_contains = ['ìš°ì„ ' , 'ë¶„ì–‘' ,'í›„', 'ì”ì—¬ì„¸ëŒ€', 'ëª¨ì§‘ê³µê³ ', 'ê³µê¸‰', 'ì„¸ëŒ€', '-', 'ì „í™˜', 'ë¸”ë¡', 'ê³µê³µ']

        # âœ… ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ëª¨ë‘ ì œê±°í•˜ëŠ” í•¨ìˆ˜
        def remove_parentheses(text):
            """ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ëª¨ë‘ ì œê±°"""
            return re.sub(r'\(.*?\)', '', text).strip()

        # âœ… í•„í„°ë§ ì¡°ê±´ í•¨ìˆ˜
        def filter_conditions(word):
            """ë‹¨ì–´ê°€ ì œê±° ëŒ€ìƒì¸ì§€ í™•ì¸"""
            # if re.search(r'[A-Za-z]', word):  # ì˜ì–´ í¬í•¨ ë‹¨ì–´ ì œê±°
            #     return False
            if word in custom_stopwords:  # íŠ¹ì • í‚¤ì›Œë“œ ì œê±°
                return False
            if any(sub in word for sub in remove_if_contains):  # íŠ¹ì • ë‹¨ì–´ í¬í•¨ ë‹¨ì–´ ì œê±°
                return False
            return True  # ëª¨ë“  ì¡°ê±´ì„ í†µê³¼í•˜ë©´ ìœ ì§€

        # âœ… ì •ì œ í•¨ìˆ˜
        def clean_name(address):
            # ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±°
            address = remove_parentheses(address)

            words = address.split()  # ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
            cleaned_words = [word for word in words if filter_conditions(word)]  # í•„í„°ë§ ì ìš©
            cleaned_address = ' '.join(cleaned_words)  # ì •ì œëœ ë‹¨ì–´ë“¤ ë‹¤ì‹œ í•©ì¹˜ê¸°

            # ì‰¼í‘œê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ë°˜í™˜
            if ',' in cleaned_address:
                return cleaned_address.split(',')[0]
            return cleaned_address  # ì‰¼í‘œê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

        # âœ… ì£¼ì†Œ ì •ì œ ì ìš©
        df['ì •ì œëœì£¼íƒëª…'] = df['ê³µê¸‰ì§€ì—­ëª…'] + ' ' + df['ì£¼íƒëª…'].apply(clean_name)

        return df

    # ì£¼ì†Œ ì „ì²˜ë¦¬ í•¨ìˆ˜
    def preprocess_address(df):
        # íŠ¹ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì •í™•í•œ ì¼ì¹˜ ë‹¨ì–´ ì œê±°)
        custom_stopwords = {'ë‚´', 'ì™¸'}

        # íŠ¹ì • ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë‹¨ì–´ ì œê±° (ì˜ˆ: 'ì§€êµ¬'ê°€ í¬í•¨ëœ ë‹¨ì–´ ì œê±°)
        remove_if_contains = ['ì§€êµ¬', 'ì‚¬ì—…', 'ì¼ì›', 'ì¼ëŒ€', 'ë¸”ë¡', 'ê³µë™', 'ì¼ë°˜', 'ê°œë°œ', 'í•„ì§€', 'êµ¬ì—­', 'ì—­ì„¸ê¶Œ', 'ë¸”ëŸ­', 'ì£¼ìƒë³µí•©', 'ì‹œí‹°', 'ì‹ ë„ì‹œ', 'ì¢…ì „', 'ë¶€ë™ì‚°']

        # âœ… ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ëª¨ë‘ ì œê±°í•˜ëŠ” í•¨ìˆ˜
        def remove_parentheses(text):
            """ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ëª¨ë‘ ì œê±°"""
            return re.sub(r'\(.*?\)', '', text).strip()

        # âœ… í•„í„°ë§ ì¡°ê±´ í•¨ìˆ˜
        def filter_conditions(word):
            """ë‹¨ì–´ê°€ ì œê±° ëŒ€ìƒì¸ì§€ í™•ì¸"""
            if re.search(r'[A-Za-z]', word):  # ì˜ì–´ í¬í•¨ ë‹¨ì–´ ì œê±°
                return False
            if word in custom_stopwords:  # íŠ¹ì • í‚¤ì›Œë“œ ì œê±°
                return False
            if any(sub in word for sub in remove_if_contains):  # íŠ¹ì • ë‹¨ì–´ í¬í•¨ ë‹¨ì–´ ì œê±°
                return False
            return True  # ëª¨ë“  ì¡°ê±´ì„ í†µê³¼í•˜ë©´ ìœ ì§€

        # âœ… ì •ì œ í•¨ìˆ˜
        def clean_address(address):
            # ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±°
            address = remove_parentheses(address)

            words = address.split()  # ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
            cleaned_words = [word for word in words if filter_conditions(word)]  # í•„í„°ë§ ì ìš©
            cleaned_address = ' '.join(cleaned_words)  # ì •ì œëœ ë‹¨ì–´ë“¤ ë‹¤ì‹œ í•©ì¹˜ê¸°

            # ì‰¼í‘œê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ë°˜í™˜
            if ',' in cleaned_address:
                return cleaned_address.split(',')[0]
            return cleaned_address  # ì‰¼í‘œê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

        # âœ… ì£¼ì†Œ ì •ì œ ì ìš©
        df['ì •ì œëœì£¼ì†Œ'] = df['ê³µê¸‰ìœ„ì¹˜'].apply(clean_address)

        return df

    # ì•„íŒŒíŠ¸ëª…, ì£¼ì†Œ ì „ì²˜ë¦¬
    df_future_estate_list_unique = preprocess_apartname(df_future_estate_list_unique)
    df_future_estate_list_unique = preprocess_address(df_future_estate_list_unique)

    # ì£¼íƒëª…ìœ¼ë¡œ ê³µê¸‰ìœ„ì¹˜ ì°¾ê¸°
    df_future_estate_list_unique['ê³µê¸‰ìœ„ì¹˜ by ì£¼íƒëª…'] = df_future_estate_list_unique['ì •ì œëœì£¼íƒëª…'].apply(add_address_by_apartname)
    
    # ê³µê¸‰ìœ„ì¹˜ë¡œ ìœ„ë„, ê²½ë„, ë²•ì •ë™ ì½”ë“œ ê°€ì ¸ì™€ì„œ í”¼ì³ì— ì¶”ê°€í•˜ê¸°
    df_future_estate_list_unique[['ìœ„ë„', 'ê²½ë„', 'í–‰ì •ë™ì½”ë“œ', 'ë²•ì •ë™ì½”ë“œ', 'ì‹œë„', 'ì‹œêµ°êµ¬', 'ìë©´ë™1', 'ìë©´ë™2']] = df_future_estate_list_unique.apply(
        lambda x: process_address_data(x['ì •ì œëœì£¼ì†Œ'], x['ê³µê¸‰ìœ„ì¹˜ by ì£¼íƒëª…']), axis=1, result_type='expand'
    )
    df_future_estate_list_unique = df_future_estate_list_unique[['ê³µê³ ë²ˆí˜¸', 'ìœ„ë„', 'ê²½ë„', 'í–‰ì •ë™ì½”ë“œ', 'ë²•ì •ë™ì½”ë“œ', 'ì‹œë„', 'ì‹œêµ°êµ¬', 'ìë©´ë™1', 'ìë©´ë™2']]

    # ë§¤ë¬¼ ëª©ë¡ì— ë¨¸ì§€í•´ì£¼ê¸°
    df_future_estate_list = pd.merge(df_future_estate_list, df_future_estate_list_unique, how='inner')
    
    return df_future_estate_list

def get_estate_price(estate_id):
    query = f'houseManageNo={estate_id}&pblancNo={estate_id}'
    URL = f'https://www.applyhome.co.kr/ai/aia/selectAPTLttotPblancDetail.do?{query}'

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36'
    }

    import requests
    from bs4 import BeautifulSoup

    response = requests.get(URL, headers=headers)
    response.content

    # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
    soup = BeautifulSoup(response.content, 'lxml')

    # ê³µê¸‰ê¸ˆì•¡ ì¡´ì¬í•˜ëŠ” í…Œì´ë¸” ì°¾ê¸°
    table = soup.findAll('table')[-2]

    # DataFrame ìƒì„±
    df = pd.read_html(str(table), converters={0:str, 1:str, 2:str})[0] # converters: ê° ì»¬ëŸ¼ì˜ ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜

    return df

def add_apply_price(df_future_estate_list):

    # ì²­ì•½ ë§¤ë¬¼ ëª©ë¡ id ê°€ì ¸ì˜¤ê¸°
    estate_ids = df_future_estate_list['ê³µê³ ë²ˆí˜¸'].unique().tolist()

    # ë¹ˆ ë¬¸ìì—´ ì¡´ì¬í•˜ì—¬ ì œê±°
    df_future_estate_list['ì£¼íƒí˜•'] = df_future_estate_list['ì£¼íƒí˜•'].str.strip()

    for estate_id in estate_ids:
        try:
            # ì²­ì•½ ë§¤ë¬¼ ê³µê¸‰ê¸ˆì•¡ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            df_estate_price = get_estate_price(int(estate_id))

            # ì²­ì•½ ë§¤ë¬¼ ëª©ë¡ ë°ì´í„°ì— ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ ê¸°ì¤€) ì»¬ëŸ¼ ì¶”ê°€
            for index in range(len(df_estate_price)):
                estate_type = str(df_estate_price.loc[index, 'ì£¼íƒí˜•']).strip()
                estate_price = float(df_estate_price.loc[index, 'ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ ê¸°ì¤€)']) * 10000

                mask = (df_future_estate_list['ê³µê³ ë²ˆí˜¸'] == estate_id) & (df_future_estate_list['ì£¼íƒí˜•'] == estate_type)
                df_future_estate_list.loc[mask, 'ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ ê¸°ì¤€)'] = estate_price
        except:
            print(f'{estate_id} ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')

    return df_future_estate_list

# ì‹œì„¸ì°¨ìµ ë°ì´í„° ì¶”ê°€
def add_market_profit(df):
    # ëª¨ì§‘ê³µê³ ì¼ ë…„ì›”ë³„ ê¸°ì¤€ìœ¼ë¡œ ì‹œì„¸ì°¨ìµì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì¤€ë¹„
    df['ëª¨ì§‘ê³µê³ ì¼_ë…„ì›”'] = pd.to_datetime(df['ëª¨ì§‘ê³µê³ ì¼']).dt.strftime('%Y%m').astype(int)
    df['ì „ìš©ë©´ì ë‹¹ ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ê¸°ì¤€)'] = df['ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ ê¸°ì¤€)'] / df['ì „ìš©ë©´ì ']

    # ì›”ë³„, ë²•ì •ë™ë³„ ì‹¤ê±°ë˜ê°€ í‰ê·  ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))

    real_estate_price_path = os.path.join(current_dir, "storage/raw_data/ì„œìš¸ê²½ê¸°ì¸ì²œ_ì „ì²´_ì›”ë³„_ë²•ì •ë™ë³„_ì‹¤ê±°ë˜ê°€_í‰ê· .csv")

    df_real_estate_price = pd.read_csv(real_estate_price_path, encoding='cp949')

    # ê° ë§¤ë¬¼ë³„ ì‹œì„¸ì°¨ìµ ê³„ì‚° í›„ ì €ì¥
    def apply_price_diff(row):
        b_code = row['ë²•ì •ë™ì½”ë“œ']
        date = row['ëª¨ì§‘ê³µê³ ì¼_ë…„ì›”']
        offer_price = row['ì „ìš©ë©´ì ë‹¹ ê³µê¸‰ê¸ˆì•¡(ìµœê³ ê°€ê¸°ì¤€)']

        mask = (df_real_estate_price['ë²•ì •ë™ì½”ë“œ'] == b_code) & (df_real_estate_price['ë…„ì›”'] == date)
        matched_rows = df_real_estate_price[mask]

        if matched_rows.empty:
            # ë§¤ì¹­ëœ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ê°’ ì²˜ë¦¬ (ì˜ˆ: NaN)
            return np.nan

        real_price = matched_rows.iloc[0]['ì „ìš©ë©´ì ë‹¹ ê±°ë˜ê¸ˆì•¡(ë§Œì›)']
        price_diff = offer_price - real_price

        return price_diff
    df['ì „ìš©ë©´ì ë‹¹ ì‹œì„¸ì°¨ìµ'] = df.apply(apply_price_diff, axis=1)

    # ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ ì œê±°
    df.drop(columns='ëª¨ì§‘ê³µê³ ì¼_ë…„ì›”', inplace=True)

    return df

def get_dummy_estate_list():
    import pandas as pd
    import os
    
    # âœ… íŒŒì¼ ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, "storage/raw_data/ë³‘í•©_ì²­ì•½ë§¤ë¬¼_ëª©ë¡_ì •ë³´_í”½ìŠ¤2.csv")
    # âœ… íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(file_path):
        print(f"ğŸš¨ íŒŒì¼ ì—†ìŒ: {file_path}")
        return pd.DataFrame()  # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜ (ì˜ˆì™¸ ë°©ì§€)

    # âœ… CSV íŒŒì¼ ë¡œë“œ (ì¸ì½”ë”© ì˜¤ë¥˜ ëŒ€ë¹„)
    try:
        df = pd.read_csv(file_path, encoding="cp949")
    except UnicodeDecodeError:
        print("âš ï¸ `cp949` ì¸ì½”ë”© ì˜¤ë¥˜ ë°œìƒ â†’ `utf-8-sig`ë¡œ ì¬ì‹œë„")
        df = pd.read_csv(file_path, encoding="utf-8-sig")

    file_path_index = os.path.join(current_dir, "storage/raw_data/ê¸°ì¤€ê¸ˆë¦¬í‘œ.csv")
    # âœ… íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(file_path_index):
        print(f"ğŸš¨ íŒŒì¼ ì—†ìŒ: {file_path_index}")
        return pd.DataFrame()  # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜ (ì˜ˆì™¸ ë°©ì§€)

    # âœ… CSV íŒŒì¼ ë¡œë“œ (ì¸ì½”ë”© ì˜¤ë¥˜ ëŒ€ë¹„)
    try:
        df_index = pd.read_csv(file_path_index, encoding="cp949")
    except UnicodeDecodeError:
        print("âš ï¸ `cp949` ì¸ì½”ë”© ì˜¤ë¥˜ ë°œìƒ â†’ `utf-8-sig`ë¡œ ì¬ì‹œë„")
        df_index = pd.read_csv(file_path_index, encoding="utf-8-sig")

    # ê¸°ì¤€ê¸ˆë¦¬ ì¶”ê°€
    df['ëª¨ì§‘ê³µê³ ì¼_tmp'] = df['ëª¨ì§‘ê³µê³ ì¼'].str[:7]
    df = df.merge(df_index, left_on='ëª¨ì§‘ê³µê³ ì¼_tmp', right_on='ë³€ê²½ì¼ì', how='left').drop(columns=['ë³€ê²½ì¼ì'])
    df.drop(columns=['ëª¨ì§‘ê³µê³ ì¼_tmp'], inplace=True)

    # âœ… ëª¨ì§‘ê³µê³ ì¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    df["ëª¨ì§‘ê³µê³ ì¼"] = pd.to_datetime(df["ëª¨ì§‘ê³µê³ ì¼"])

    # âœ… 2025-03-12 ì´ìƒì¸ ë§¤ë¬¼ë§Œ í•„í„°ë§
    filtered_df = df[df["ëª¨ì§‘ê³µê³ ì¼"] >= "2025-01-01"]

    # ì‹œì„¸ì°¨ìµ NaN ì²˜ë¦¬
    filtered_df['ì‹œì„¸ì°¨ìµ'] = np.nan

    # ë‹¹ì²¨ê°€ì  Nan ì²˜ë¦¬
    filtered_df['ìµœì €ë‹¹ì²¨ê°€ì '] = np.nan
    filtered_df['ìµœê³ ë‹¹ì²¨ê°€ì '] = np.nan
    filtered_df['í‰ê· ë‹¹ì²¨ê°€ì '] = np.nan

    # ê²½ìŸë¥  ì²˜ë¦¬
    def preprocessing_applicant_rate(df):
        # ê²½ìŸë¥ ì—ì„œ '-' ë¥¼ NaNìœ¼ë¡œ ë°”ê¾¸ê³ , NaNì„ ëª¨ë‘ 0.0ìœ¼ë¡œ ë³€í™˜
        df["ê²½ìŸë¥ "] = df["ê²½ìŸë¥ "].replace("-", np.nan).fillna(0.0)

        def process_rate(row):
            # ë¯¸ë‹¬ì¸ ê²½ìš° ê²½ìŸë¥  ì²˜ë¦¬
            if "â–³" in str(row["ê²½ìŸë¥ "]):
                pattern = "[^0-9]"
                shortage = int(re.sub(pattern, "", str(row["ê²½ìŸë¥ "])))

                supply_units = float(row["ê³µê¸‰ì„¸ëŒ€ìˆ˜"])
                if supply_units == 0 or int(row["ì ‘ìˆ˜ê±´ìˆ˜"]) == 0:
                    rate = 0.0
                else:
                    rate = round((supply_units - shortage) / supply_units, 2)
            else:
                rate = float(str(row["ê²½ìŸë¥ "]).replace(",", ""))

            # ë¯¸ë‹¬ì—¬ë¶€ íŒë‹¨
            # shortage_status = "Y" if rate < 1 else "N"

            return pd.Series({"ê²½ìŸë¥ ": rate })
        
        df[["ê²½ìŸë¥ "]] = df.apply(process_rate, axis=1)

        return df
    
    filtered_df = preprocessing_applicant_rate(filtered_df)

    filtered_df = filtered_df[filtered_df['ê²½ìŸë¥ '] >= 1]

    filtered_df['ì „ìš©ë©´ì '] = filtered_df['ì£¼íƒí˜•'].apply(lambda x: ''.join(filter(lambda y: y.isdigit() or y == '.', x)))
    filtered_df['ì „ìš©ë©´ì '] = filtered_df['ì „ìš©ë©´ì '].astype(float)

    filtered_df = filtered_df[filtered_df['ì „ìš©ë©´ì '] <= 85]

    def preprocess_apartname(df):

        # íŠ¹ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì •í™•í•œ ì¼ì¹˜ ë‹¨ì–´ ì œê±°)
        custom_stopwords = {}

        # íŠ¹ì • ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë‹¨ì–´ ì œê±° (ì˜ˆ: 'ì§€êµ¬'ê°€ í¬í•¨ëœ ë‹¨ì–´ ì œê±°)
        remove_if_contains = ['ìš°ì„ ' , 'ë¶„ì–‘' ,'í›„', 'ì”ì—¬ì„¸ëŒ€', 'ëª¨ì§‘ê³µê³ ', 'ê³µê¸‰', 'ì„¸ëŒ€', '-', 'ì „í™˜', 'ë¸”ë¡', 'ê³µê³µ']

        # âœ… ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ëª¨ë‘ ì œê±°í•˜ëŠ” í•¨ìˆ˜
        def remove_parentheses(text):
            """ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ëª¨ë‘ ì œê±°"""
            return re.sub(r'\(.*?\)', '', text).strip()

        # âœ… í•„í„°ë§ ì¡°ê±´ í•¨ìˆ˜
        def filter_conditions(word):
            """ë‹¨ì–´ê°€ ì œê±° ëŒ€ìƒì¸ì§€ í™•ì¸"""
            # if re.search(r'[A-Za-z]', word):  # ì˜ì–´ í¬í•¨ ë‹¨ì–´ ì œê±°
            #     return False
            if word in custom_stopwords:  # íŠ¹ì • í‚¤ì›Œë“œ ì œê±°
                return False
            if any(sub in word for sub in remove_if_contains):  # íŠ¹ì • ë‹¨ì–´ í¬í•¨ ë‹¨ì–´ ì œê±°
                return False
            return True  # ëª¨ë“  ì¡°ê±´ì„ í†µê³¼í•˜ë©´ ìœ ì§€

        # âœ… ì •ì œ í•¨ìˆ˜
        def clean_name(address):
            # ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì œê±°
            address = remove_parentheses(address)

            words = address.split()  # ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
            cleaned_words = [word for word in words if filter_conditions(word)]  # í•„í„°ë§ ì ìš©
            cleaned_address = ' '.join(cleaned_words)  # ì •ì œëœ ë‹¨ì–´ë“¤ ë‹¤ì‹œ í•©ì¹˜ê¸°

            # ì‰¼í‘œê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ë°˜í™˜
            if ',' in cleaned_address:
                return cleaned_address.split(',')[0]
            return cleaned_address  # ì‰¼í‘œê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

        # âœ… ì£¼ì†Œ ì •ì œ ì ìš©
        df['ì •ì œëœì£¼íƒëª…'] = df['ì£¼íƒëª…'].apply(clean_name)

        return df
    filtered_df = preprocess_apartname(filtered_df)
    filtered_df['ì£¼íƒëª…'] = filtered_df['ì •ì œëœì£¼íƒëª…']
    filtered_df.drop(columns=['ì •ì œëœì£¼íƒëª…'], inplace=True)

    return filtered_df
